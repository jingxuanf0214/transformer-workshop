{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fcce1c8",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/KempnerInstitute/transformer-workshop/blob/main/transformer_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJjyDCYiycRG"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Yy3ncD8pycRH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbDLagMhycRH"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rgppY_zycRH"
   },
   "source": [
    "## Exercise: Implementing Character-based tokenization\n",
    "\n",
    "1. Get a sorted list of every unique character in your training data.\n",
    "2. Create a dictionary that converts tokens to IDs (str_to_int) and one that converts IDs to tokens (int_to_str)\n",
    "3. Implement functions encode and decode.\n",
    "Encode should take in a string and output list of token IDs.\n",
    "Decode should take in a list of token IDs and output a string.\n",
    "4. Test encoding and then decoding “My dog Leo is extremely cute.” Do you recover the correct string?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JnmqJ-922AD_"
   },
   "outputs": [],
   "source": [
    "# Load in all training data\n",
    "with open('tiny_wikipedia.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pl73KMyZycRH",
    "outputId": "b7d9d16c-a7f6-4573-ceb6-1aec4a09cec0"
   },
   "outputs": [],
   "source": [
    "# Step 1) Get a sorted list of all unique characters that occur in this text\n",
    "# Hint: set is useful for getting unique elements in a sequence\n",
    "chars = sorted(list(set(text))) # your code here\n",
    "\n",
    "# Step 2) Create the dictionaries str_to_int and int_to_str\n",
    "str_to_int = {char: i for i, char in enumerate(chars)} # your code here\n",
    "int_to_str = {i: char for i, char in enumerate(chars)} # your code here\n",
    "# Step 3) Define encode and decode functions\n",
    "def encode(input_text, str_to_int):\n",
    "    return [str_to_int[char] for char in input_text]\n",
    "\n",
    "def decode(ids, int_to_str):\n",
    "    return \"\".join([int_to_str[i] for i in ids])\n",
    "# \n",
    "# Testing your implementation \n",
    "input_text = \"My dog Leo is extremely cute.\"\n",
    "ids = encode(input_text, str_to_int)\n",
    "decoded_text = decode(ids, int_to_str)\n",
    "assert input_text == decoded_text, \"Decoded text does not match input\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for hint for step 1*](https://github.com/KempnerInstitute/transformer-workshop/blob/main//hints/transformer_Hint_2fccbe47.md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCQAN_QY3zPO"
   },
   "source": [
    "[*Click for solution*](https://github.com/KempnerInstitute/transformer-workshop/blob/main//solutions/transformer_Solution_dab83af6.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUKsH64ZycRI"
   },
   "source": [
    "## Tokenize input data and create splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rVKYkgLuycRI"
   },
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text, str_to_int), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split, ctx_len, batch_size, device='cpu'):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - ctx_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+ctx_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+ctx_len+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our transformer parameters with a config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 256 # the model/hidden/embedding dim\n",
    "    n_heads: int = 4 # number of attention heads (width)\n",
    "    ctx_len: int = 64 # context length\n",
    "    batch_size: int = 8 # batch size\n",
    "    n_layers: int = 12 # number of layers (depth)\n",
    "    vocab_size: int = -1 # vocab size, to be determined once we have created a tokenizer\n",
    "    device: str = 'cpu'\n",
    "\n",
    "    def set_vocab_size(self, vocab_size):\n",
    "        self.vocab_size = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config.set_vocab_size(vocab_size=len(chars)) # set our vocabular size (equal to the number of chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Implement token embeddings\n",
    "\n",
    "We want to implement a class that will take in a batch of token IDs (batch size by context length) and output the token embeddings (batch size by context length by embedding dimensions). Find the `nn.Embedding` docs [here](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "893"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: fill out nn.Embedding arguments for token embedding\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # TODO: Get forward pass of token embedding layer\n",
    "        x_tok = self.wte(x)\n",
    "\n",
    "        return x_tok\n",
    "    \n",
    "\n",
    "# Testing your implementation\n",
    "xb, yb = get_batch('train', config.ctx_len, config.batch_size, config.device)\n",
    "\n",
    "token_embedding = TokenEmbeddingLayer(config)\n",
    "x_tok = token_embedding(xb)\n",
    "\n",
    "assert x_tok.shape == (config.batch_size, config.ctx_len, config.d_model), \"Embedding dimensions are incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/KempnerInstitute/transformer-workshop/blob/main//solutions/transformer_Solution_4d19d660.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Implement full embedding layer\n",
    "\n",
    "Now we'll write the full embedding layer including both token and position embeddings. You can use your solutions from the previous part for the token embeddings. How can you implement the position embeddings? This is a little tricky so feel free to click on the hints below the exercise for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = config.device\n",
    "\n",
    "        # TODO: fill out nn.Embedding arguments for token embedding (same as before)\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "        # TODO: implement position embedding\n",
    "        self.wpe = nn.Embedding(config.ctx_len, config.d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # TODO: Get forward pass of token and position embedding \n",
    "        x_tok = self.wte(x)\n",
    "        x_pos = self.wpe(torch.arange(T).to(self.device))\n",
    "        x_embeddings = x_tok + x_pos \n",
    "\n",
    "        return x_embeddings\n",
    "    \n",
    "\n",
    "# Testing your implementation\n",
    "xb, yb = get_batch('train', config.ctx_len, config.batch_size, config.device)\n",
    "\n",
    "embedding = EmbeddingLayer(config)\n",
    "x_embedding = embedding(xb)\n",
    "\n",
    "assert x_embedding.shape == (config.batch_size, config.ctx_len, config.d_model), \"Embedding dimensions are incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For position embedding it's not batch by context_len by embedding dim (because you don't need one pos embedding for each token, it only cares about position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for hint #1*](https://github.com/KempnerInstitute/transformer-workshop/blob/main//hints/transformer_Hint_c048a49d.md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for hint #2*](https://github.com/KempnerInstitute/transformer-workshop/blob/main//hints/transformer_Hint_59a42f44.md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/KempnerInstitute/transformer-workshop/blob/main//solutions/transformer_Solution_e78fa771.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbNpCKoSycRI"
   },
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Implementing single headed causal self attention\n",
    "\n",
    "Self-attention is a core mechanism in transformers that allows each position in a sequence to attend to all previous positions. The \"causal\" part ensures each position can only attend to past positions - this is crucial for language modeling.\n",
    "\n",
    "The task below is to fill out the `SingleHeadCausalAttention` module.  The `__init__` method should define the key, query, and value projections.  Note that the causal mask has been already defined for you (it is a lower triangular matrix whose entries are 1's.  You can refer to it by calling `self.cmask`.)\n",
    "\n",
    "The `forward(self, x)` function that will take in an input `x` that is `(B, T, C)`-dimensional corresponding to batch size, sequence length, and hidden dimension and then output the result after applying the attention formula.  To do this,\n",
    "\n",
    "1. Create the K, Q, V matrices that are the resultant matrices after applying the `self.key`, `self.query`, and `self.values` projections.\n",
    "2. Compute and return attention using the formula:\n",
    "\n",
    "$$\\textrm{attention}(K, V, Q) = \\textrm{softmax}\\left( c \\odot \\frac{Q K^\\top}{\\sqrt{d_k}} \\right) V $$\n",
    "\n",
    "where $c \\odot \\dots$ denotes the application of the causal mask.  You can use `torch.masked_fill(...)` here to apply the mask.  It takes as input three arguments: the input matrix you want to mask, where you want to mask it (a boolean condition), and the value you want to mask with.  To figure out what value you want to mask with, it may be helpful to recall the softmax formula; the $i$-th component of a vector $u$ after a softmax is: $$ \\textrm{softmax}(x)_i =  \\frac{e^{x_i}}{\\sum_j e^{x_j}}.$$\n",
    "\n",
    "Hints:\n",
    "1. Keep track of the matrix dimensions after each step!\n",
    "2. Note that you can transpose a matrix in Pytorch by calling `A.transpose(dim_1, dim_2)` where `dim_1`, `dim_2` refer to the dimensions you want to transpose.\n",
    "3. You may use Pytorch's built-in softmax function `F.softmax(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadCausalAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calculate the dimension for each attention head\n",
    "        self.head_dim = config.d_model // config.n_heads\n",
    "        \n",
    "        # TODO: Initialize the Key, Query, and Value projections\n",
    "        # Each should be a linear layer that projects from d_model to head_dim\n",
    "        # Hint: Use nn.Linear(..., bias=False) as is standard in attention\n",
    "        self.key = nn.Linear(config.d_model, self.head_dim, bias=False) # Your code here\n",
    "        self.query = nn.Linear(config.d_model, self.head_dim, bias=False) # Your code here\n",
    "        self.values = nn.Linear(config.d_model, self.head_dim, bias=False) # Your code here\n",
    "        \n",
    "        # Create causal mask (lower triangular matrix), you an refer to it by `self.cmask`\n",
    "        self.register_buffer(\"cmask\", torch.tril(torch.ones([config.ctx_len, config.ctx_len])))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # TODO Step 1: Project input to get Key, Query, Value matrices\n",
    "        K = self.key(x) # Your code here (get B, T, head_dim)\n",
    "        Q = self.query(x) # Your code here\n",
    "        V = self.values(x) # Your code here\n",
    "        #print(K.shape)\n",
    "        # TODO Step 2: Compute attention scores and apply mask\n",
    "        # Remember: \n",
    "        # - Scale by sqrt(head_dim)\n",
    "        # - Use the causal mask (self.cmask) to prevent attention to future tokens (you can use `torch.masked_fill(...)` here)\n",
    "        # - Apply softmax to get attention weights\n",
    "        # - Multiply with values\n",
    "        # transpose only on the last two dimensions (we don't want transpose batch)\n",
    "        dot_product  = Q @ K.transpose(-2,-1)*self.head_dim**0.5\n",
    "        # we mask the things where it's zero in the causal mask, we mask by -inf (e to -inf -> attantion score 0)\n",
    "        masked_dot = dot_product.masked_fill(self.cmask[None, :T, :T] == 0, float('-inf'))\n",
    "        #print(masked_dot.shape)\n",
    "        # sofftmax in last dimension -> sequence length \n",
    "        att_score = F.softmax(masked_dot,dim=-1) @ V# Your implementation here...\n",
    "        \n",
    "        return att_score# Final output\n",
    "    \n",
    "\n",
    "# Test your implementation\n",
    "config = Config(d_model=256, n_heads=8, ctx_len=16)\n",
    "attention = SingleHeadCausalAttention(config)\n",
    "x = torch.randn(2, 10, 256)  # (batch_size, seq_len, d_model)\n",
    "output = attention(x)\n",
    "assert output.shape == (2, 10, 32)  # head_dim = 256/8 = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/KempnerInstitute/transformer-workshop/blob/main//solutions/transformer_Solution_ebadd322.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: implementing multi-head attention\n",
    "\n",
    "The task is to write the multi-headed self attention module.  You should not need to write more than a few lines of code here.\n",
    "\n",
    "1. Define `self.heads` as the list of attention heads that will act in parallel on the input.  You may use `nn.ModuleList(...)` to do this.\n",
    "2. Define `self.linear`, a linear projection.\n",
    "3. Define the forward function which will take in the input `x` (which is (B, T, C)-dimesional), pass it through each head, and concatenate the output.  To perform concatenation you can use `torch.cat(...)`.\n",
    "4. After going through the attention heads, the input should then go through the linear projection and then returned at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(SingleHeadCausalAttention(config) for _ in range(config.n_heads)) # your code here, you can define the heads using `nn.ModuleList(...)`\n",
    "        self.linear = nn.Linear(config.d_model, config.d_model) # your code here, with `d_model` in-features and `d_model` out-features\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: fill out the forward method for multi-head attention\n",
    "        # Remember:\n",
    "        # - pass input through all heads and concatenate the output (you can use `torch.cat(...)` here)\n",
    "        # - pass the result through the linear layer and return the output\n",
    "        \n",
    "        y = torch.cat([h(x) for h in self.heads], dim=-1) # your code here\n",
    "        y = self.linear(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "# Testing your implementation\n",
    "config = Config(d_model=256, n_heads=8, ctx_len=16)\n",
    "mha = MultiHeadCausalAttention(config)\n",
    "\n",
    "# Test with small batch\n",
    "x = torch.randn(2, 10, 256)  # (batch_size=2, seq_len=10, d_model=256)\n",
    "out = mha(x)\n",
    "assert out.shape == (2, 10, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/KempnerInstitute/transformer-workshop/blob/main//solutions/transformer_Solution_3ed91119.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the feed-forward network (FFN) decoder block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: FFN\n",
    "\n",
    "The Feed-Forward Network (FFN) is a simple yet powerful component that applies two linear transformations with a ReLU activation in between. The first transformation expands the input dimension by a factor of 4, and the second transformation projects it back to the original dimension.  In this exercise, you will implement this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # TODO: Initialize two linear layers\n",
    "        # First layer should expand from d_model to 4*d_model\n",
    "        # Second layer should project back to d_model\n",
    "        # Hint: use nn.Linear(in_features, out_features)\n",
    "        self.l1 = nn.Linear(config.d_model, 4*config.d_model) # Your code here\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(4*config.d_model, config.d_model) # Your code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        # 1. Apply first linear layer\n",
    "        # 2. Apply ReLU activation\n",
    "        # 3. Apply second linear layer\n",
    "        x = self.l1(x) # Your code here\n",
    "        x = self.relu(x) # Your code here\n",
    "        x = self.l2(x) # Your code here\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/KempnerInstitute/transformer-workshop/blob/main//solutions/transformer_Solution_a3d55905.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Decoder Block\n",
    "\n",
    "The Decoder Block is a core component that combines self-attention with a feed-forward network. It uses residual connections and layer normalization to help with training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadCausalAttention(config)\n",
    "        # TODO: Initialize layer normalization layers\n",
    "        # Hint: use nn.LayerNorm(config.d_model)\n",
    "        self.ln1 = nn.LayerNorm(config.d_model) # Your code here\n",
    "        self.ffn = FFN(config)\n",
    "        self.ln2 = nn.LayerNorm(config.d_model) # Your code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass with residual connections\n",
    "        # Remember the pattern: x = x + sublayer(layer_norm(x))\n",
    "        x = x + self.mha(self.ln1(x)) # Your code here  # First attention block with residual\n",
    "        x = x + self.ffn(self.ln2(x)) # Your code here  # Second FFN block with residual\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Testing your implementation\n",
    "config = Config(d_model=256)\n",
    "ffn = FFN(config)\n",
    "decoder = DecoderBlock(config)\n",
    "\n",
    "# Test with random input\n",
    "x = torch.randn(2, 10, 256)  # (batch_size, sequence_length, d_model)\n",
    "output = decoder(x)\n",
    "assert output.shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/KempnerInstitute/transformer-workshop/blob/main//solutions/transformer_Solution_0c7ded67.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the transformer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to put the components together into our final decoder module that can actually generate text! Your task to implement the missing pieces of the Decoder class. This is the top-level module that:\n",
    "\n",
    "* Embeds input tokens and adds positional information\n",
    "* Processes them through multiple transformer layers\n",
    "* Outputs predictions for the next token through the `forward(...)` function\n",
    "* Can generate new sequences autoregressively through the `generate(...)` function\n",
    "\n",
    "We have given extra hints for this module since it is a challenging exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(d_model=256, n_heads=4, ctx_len=64, batch_size=8, n_layers=12, vocab_size=-1, device='cpu')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Stack of decoder blocks\n",
    "        self.blocks = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layers)])\n",
    "\n",
    "        # TODO: Initialize components\n",
    "        # Final layer norm and projection to vocabulary\n",
    "        self.ln = nn.LayerNorm(config.d_model) # normalize to d_model dimension\n",
    "        self.lin = nn.Linear(config.d_model, config.vocab_size) # project from d_model to vocab_size\n",
    "        \n",
    "        # Embeddings\n",
    "        self.emb = EmbeddingLayer(config)\n",
    "\n",
    "        # Loss function for training\n",
    "        self.L = nn.CrossEntropyLoss()\n",
    "        self.ctx_len = config.ctx_len\n",
    "\n",
    "        self.device = config.device # don't change this (for training model on right device)\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tokens (B, T)\n",
    "            targets: Optional target tokens (B, T)\n",
    "        Returns:\n",
    "            logits: Predictions (B, T, vocab_size)\n",
    "            loss: Optional cross-entropy loss\n",
    "        \"\"\"\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # Step 1: Get embeddings\n",
    "        x = self.emb(x) \n",
    "        \n",
    "        # TODO Step 2: Process through transformer\n",
    "        x = self.blocks(x)          # Apply transformer blocks\n",
    "        x = self.ln(x) # Your code here        # Apply final layer norm\n",
    "        logits = self.lin(x) # Your code here   # Project to vocabulary size\n",
    "        \n",
    "        # TODO Step 3: Compute loss if targets are provided\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshape logits and targets for loss computation\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)    # Combine batch and time dimensions\n",
    "            targets = targets.view(B*T)      # Flatten targets\n",
    "            loss = self.L(logits,targets) # Your code here          # Compute cross entropy loss\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_len=256):\n",
    "        \"\"\"\n",
    "        Generate new tokens given initial sequence idx.\n",
    "        \"\"\"\n",
    "        # TODO: Implement generation loop\n",
    "        for _ in range(max_len):\n",
    "            # Step 1: Take the last ctx_len tokens (max size of context)\n",
    "            idx_window = idx[:,-config.ctx_len:] # Your code here\n",
    "            \n",
    "            # Step 2: Get model predictions\n",
    "            logits, _ = self(idx_window)     # (B, T, V)\n",
    "            logits = logits[:, -1, :]        # Only take the last token's predictions\n",
    "            \n",
    "            # Step 3: Sample next token\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Step 4: Append to sequence\n",
    "            idx = torch.cat([idx,next_token],dim=1) # Your code here\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "\n",
    "# Testing your implementation\n",
    "config = Config(\n",
    "    vocab_size=100,\n",
    "    d_model=256,\n",
    "    ctx_len=64,\n",
    "    n_layers=4\n",
    ")\n",
    "decoder = Decoder(config)\n",
    "\n",
    "x = torch.randint(0, 100, (1, 10))\n",
    "logits, loss = decoder(x, x)\n",
    "\n",
    "out = decoder.generate(torch.tensor([[1, 2, 3]]), max_len=5)\n",
    "print(out.shape)  # Should be (1, 8) - original 3 tokens + 5 new ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/KempnerInstitute/transformer-workshop/blob/main//solutions/transformer_Solution_1af838dc.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(d_model=768, n_heads=12, ctx_len=512, batch_size = 64, n_layers = 12, device='cuda:0')\n",
    "config.set_vocab_size(vocab_size=len(chars))\n",
    "model = Decoder(config).to(config.device)\n",
    "\n",
    "# print the size of the model\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total model parameters: {n_params}\")\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 3e-4\n",
    "max_iters = 6000\n",
    "eval_interval = 200  # How often to evaluate\n",
    "eval_iters = 100     # How many batches to use for evaluation\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    xb, yb = get_batch('train', config.ctx_len, config.batch_size, config.device)\n",
    "    \n",
    "    # forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    # backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # evaluate on val data at specified intervals\n",
    "    if iter % eval_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for _ in range(eval_iters):\n",
    "                xb, yb = get_batch('val', config.ctx_len, config.batch_size, config.device)\n",
    "                _, val_loss = model(xb, yb)\n",
    "                val_losses.append(val_loss.item())\n",
    "            val_loss = sum(val_losses) / len(val_losses)\n",
    "            \n",
    "            print(f\"step {iter}: train loss {loss.item():.4f}, val loss {val_loss:.4f}\")\n",
    "            \n",
    "            # generate some text to see how we're doing\n",
    "            context = torch.zeros((1, 1), dtype=torch.long, device=config.device)\n",
    "            print(decode(model.generate(context, max_len=100)[0].tolist(), int_to_str))\n",
    "            print('='*40)\n",
    "        model.train()\n",
    "\n",
    "# Final generation\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=config.device)\n",
    "print(\"\\nFinal generated text:\")\n",
    "print(decode(model.generate(context, max_len=500)[0].tolist(), int_to_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our model\n",
    "import os\n",
    "if not os.path.exists('model.pth'):\n",
    "   torch.save(model.state_dict(), 'model.pth')\n",
    "else:\n",
    "   print('Model file (model.pth) already exists!  Saving under a different name model_other.pth.')\n",
    "   torch.save(model.state_dict(), 'model_other.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load a saved model, uncomment the below code\n",
    "# model = Decoder(config)\n",
    "# model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "config.device = 'cuda:0'  # change this to 'cpu' if you don't have access to a GPU\n",
    "model.eval()\n",
    "context = torch.tensor([encode(\"The Roman Empire lasted from\", str_to_int)], dtype=torch.long, device=config.device)\n",
    "print(decode(model.generate(context, max_len=512)[0].tolist(), int_to_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.tensor([encode(\"Transformer models are described by\", str_to_int)], dtype=torch.long, device=config.device)\n",
    "print(decode(model.generate(context, max_len=512)[0].tolist(), int_to_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
