{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJjyDCYiycRG"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yy3ncD8pycRH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbDLagMhycRH"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rgppY_zycRH"
      },
      "source": [
        "## Exercise: Implementing Character-based tokenization\n",
        "\n",
        "1. Get a sorted list of every unique character in your training data.\n",
        "2. Create a dictionary that converts tokens to IDs (str_to_int) and one that converts IDs to tokens (int_to_str)\n",
        "3. Implement functions encode and decode.\n",
        "Encode should take in a string and output list of token IDs.\n",
        "Decode should take in a list of token IDs and output a string.\n",
        "4. Test encoding and then decoding ‚ÄúMy dog Leo is extremely cute.‚Äù Do you recover the correct string?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JnmqJ-922AD_"
      },
      "outputs": [],
      "source": [
        "# Load in all training data\n",
        "with open('tiny_wikipedia.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl73KMyZycRH",
        "outputId": "b7d9d16c-a7f6-4573-ceb6-1aec4a09cec0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 1) Get a sorted list of all unique characters that occur in this text\n",
        "# Hint: set is useful for getting unique elements in a sequence\n",
        "... # your code here\n",
        "\n",
        "# Step 2) Create the dictionaries str_to_int and int_to_str\n",
        "... # your code here\n",
        "\n",
        "# Step 3) Define encode and decode functions\n",
        "# def encode(...):\n",
        "#     ...\n",
        "\n",
        "# def decode(...):\n",
        "#     ...\n",
        "\n",
        "# Step 4) Test your implementation on \"My dog Leo is extremely cute.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRkLNO4B4hKq"
      },
      "source": [
        "# Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pCQAN_QY3zPO"
      },
      "outputs": [],
      "source": [
        "# Step 1) Get a sorted list of all unique characters that occur in this text\n",
        "# Hint: set is useful for getting unique elements in a sequence\n",
        "chars = sorted(list(set(text)))\n",
        "\n",
        "# Step 2) Create the dictionaries str_to_int and int_to_str\n",
        "str_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_str = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Step 3) Define encode and decode functions\n",
        "def encode(text, str_to_int):\n",
        "    ids = [str_to_int[c] for c in text]\n",
        "    return ids\n",
        "\n",
        "def decode(ids, int_to_str):\n",
        "    text_list = [int_to_str[id] for id in ids]\n",
        "    return ''.join(text_list)\n",
        "\n",
        "# Step 4) Test your implementation on \"My dog Leo is extremely cute.\"\n",
        "input_text = \"My dog Leo is extremely cute.\"\n",
        "ids = encode(input_text, str_to_int)\n",
        "decoded_text = decode(ids, int_to_str)\n",
        "assert input_text == decoded_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUKsH64ZycRI"
      },
      "source": [
        "## Tokenize input data and create splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rVKYkgLuycRI"
      },
      "outputs": [],
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text, str_to_int), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split, ctx_len, batch_size, device='cpu'):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - ctx_len, (batch_size,))\n",
        "    x = torch.stack([data[i:i+ctx_len] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+ctx_len+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define our transformer parameters with a config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    d_model: int = 256 # the model/hidden/embedding dim\n",
        "    n_heads: int = 4 # number of attention heads (width)\n",
        "    ctx_len: int = 64 # context length\n",
        "    batch_size: int = 8 # batch size\n",
        "    n_layers: int = 12 # number of layers (depth)\n",
        "    vocab_size: int = -1 # vocab size, to be determined once we have created a tokenizer\n",
        "    device: str = 'cpu'\n",
        "\n",
        "    def set_vocab_size(self, vocab_size):\n",
        "        self.vocab_size = vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = Config()\n",
        "config.set_vocab_size(vocab_size=len(chars)) # set our vocabular size (equal to the number of chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbNpCKoSycRI"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise: Implementing single headed causal self attention\n",
        "\n",
        "Self-attention is a core mechanism in transformers that allows each position in a sequence to attend to all previous positions. The \"causal\" part ensures each position can only attend to past positions - this is crucial for language modeling.\n",
        "\n",
        "The task below is to fill out the `SingleHeadCausalAttention` module.  The `__init__` method should define the key, query, and value projections.  Note that the causal mask has been already defined for you (it is a lower triangular matrix whose entries are 1's.  You can refer to it by calling `self.cmask`.)\n",
        "\n",
        "The `forward(self, x)` function that will take in an input `x` that is `(B, T, C)`-dimensional corresponding to batch size, sequence length, and hidden dimension and then output the result after applying the attention formula.  To do this,\n",
        "\n",
        "1. Create the K, Q, V matrices that are the resultant matrices after applying the `self.key`, `self.query`, and `self.values` projections.\n",
        "2. Compute and return attention using the formula:\n",
        "\n",
        "$$\\textrm{attention}(K, V, Q) = \\textrm{softmax}\\left( c \\odot \\frac{Q K^\\top}{\\sqrt{d_k}} \\right) V $$\n",
        "\n",
        "where $c \\odot \\dots$ denotes the application of the causal mask.  You can use `torch.masked_fill(...)` here to apply the mask.  It takes as input three arguments: the input matrix you want to mask, where you want to mask it (a boolean condition), and the value you want to mask with.  To figure out what value you want to mask with, it may be helpful to recall the softmax formula; the $i$-th component of a vector $u$ after a softmax is: $$ \\textrm{softmax}(x)_i =  \\frac{e^{x_i}}{\\sum_j e^{x_j}}.$$\n",
        "\n",
        "Hints:\n",
        "1. Keep track of the matrix dimensions after each step!\n",
        "2. Note that you can transpose a matrix in Pytorch by calling `A.transpose(dim_1, dim_2)` where `dim_1`, `dim_2` refer to the dimensions you want to transpose.\n",
        "3. You may use Pytorch's built-in softmax function `F.softmax(...)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SingleHeadCausalAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Calculate the dimension for each attention head\n",
        "        self.head_dim = config.d_model // config.n_heads\n",
        "        \n",
        "        # TODO: Initialize the Key, Query, and Value projections\n",
        "        # Each should be a linear layer that projects from d_model to head_dim\n",
        "        # Hint: Use nn.Linear(..., bias=False) as is standard in attention\n",
        "        self.key = ... # Your code here\n",
        "        self.query = ... # Your code here\n",
        "        self.values = ... # Your code here\n",
        "        \n",
        "        # Create causal mask (lower triangular matrix), you an refer to it by `self.cmask`\n",
        "        self.register_buffer(\"cmask\", torch.tril(torch.ones([config.ctx_len, config.ctx_len])))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        \n",
        "        # TODO Step 1: Project input to get Key, Query, Value matrices\n",
        "        K = ... # Your code here\n",
        "        Q = ... # Your code here\n",
        "        V = ... # Your code here\n",
        "        \n",
        "        # TODO Step 2: Compute attention scores and apply mask\n",
        "        # Remember: \n",
        "        # - Scale by sqrt(head_dim)\n",
        "        # - Use the causal mask (self.cmask) to prevent attention to future tokens (you can use `torch.masked_fill(...)` here)\n",
        "        # - Apply softmax to get attention weights\n",
        "        # - Multiply with values\n",
        "        \n",
        "        ... # Your implementation here...\n",
        "        \n",
        "        return # Final output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = Config(d_model=256, n_heads=8, ctx_len=16)\n",
        "attention = SingleHeadCausalAttention(config)\n",
        "x = torch.randn(2, 10, 256)  # (batch_size, seq_len, d_model)\n",
        "output = attention(x)\n",
        "assert output.shape == (2, 10, 32)  # head_dim = 256/8 = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SingleHeadCausalAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head_dim = config.d_model // config.n_heads\n",
        "        self.key = nn.Linear(config.d_model, self.head_dim, bias=False)\n",
        "        self.query = nn.Linear(config.d_model, self.head_dim, bias=False)\n",
        "        self.values = nn.Linear(config.d_model, self.head_dim, bias=False)\n",
        "\n",
        "        self.register_buffer(\"cmask\", torch.tril(torch.ones([config.ctx_len, config.ctx_len])))\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C = x.shape\n",
        "        \n",
        "        K = self.key(x) # (B, T, C) @ (_, C, H) -> (B, T, H)\n",
        "        Q = self.query(x)\n",
        "        V = self.values(x)\n",
        "\n",
        "        y = Q @ K.transpose(-2, -1) * self.head_dim**-0.5 # (B, T, H) @ (B, H, T) -> (B, T, T)\n",
        "        y = torch.masked_fill(y, self.cmask[:T, :T]==0, float('-inf'))\n",
        "        y = F.softmax(y, dim=-1) @ V\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-head self attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise: implementing multi-head attention\n",
        "\n",
        "The task is to write the multi-headed self attention module.  You should not need to write more than a few lines of code here.\n",
        "\n",
        "1. Define `self.heads` as the list of attention heads that will act in parallel on the input.  You may use `nn.ModuleList(...)` to do this.\n",
        "2. Define `self.linear`, a linear projection.\n",
        "3. Define the forward function which will take in the input `x` (which is (B, T, C)-dimesional), pass it through each head, and concatenate the output.  To perform concatenation you can use `torch.cat(...)`.\n",
        "4. After going through the attention heads, the input should then go through the linear projection and then returned at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadCausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.heads = ... # your code here, you can define the heads using `nn.ModuleList(...)`\n",
        "        self.linear = ... # your code here, with `d_model` in-features and `d_model` out-features\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: fill out the forward method for multi-head attention\n",
        "        # Remember:\n",
        "        # - pass input through all heads and concatenate the output (you can use `torch.cat(...)` here)\n",
        "        # - pass the result through the linear layer and return the output\n",
        "        \n",
        "        ... # your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = Config(d_model=256, n_heads=8, ctx_len=16)\n",
        "mha = MultiHeadCausalAttention(config)\n",
        "\n",
        "# Test with small batch\n",
        "x = torch.randn(2, 10, 256)  # (batch_size=2, seq_len=10, d_model=256)\n",
        "out = mha(x)\n",
        "assert out.shape == (2, 10, 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadCausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([SingleHeadCausalAttention(config) for _ in range(config.n_heads)])\n",
        "        self.linear = nn.Linear(config.d_model, config.d_model)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        y = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        y = self.linear(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define the feed-forward network (FFN) decoder block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise: FFN\n",
        "\n",
        "The Feed-Forward Network (FFN) is a simple yet powerful component that applies two linear transformations with a ReLU activation in between. The first transformation expands the input dimension by a factor of 4, and the second transformation projects it back to the original dimension.  In this exercise, you will implement this module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # TODO: Initialize two linear layers\n",
        "        # First layer should expand from d_model to 4*d_model\n",
        "        # Second layer should project back to d_model\n",
        "        # Hint: use nn.Linear(in_features, out_features)\n",
        "        self.l1 = ... # Your code here\n",
        "        self.relu = nn.ReLU()\n",
        "        self.l2 = ... # Your code here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement the forward pass\n",
        "        # 1. Apply first linear layer\n",
        "        # 2. Apply ReLU activation\n",
        "        # 3. Apply second linear layer\n",
        "        x = ... # Your code here\n",
        "        x = ... # Your code here\n",
        "        x = ... # Your code here\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise: Decoder Block\n",
        "\n",
        "The Decoder Block is a core component that combines self-attention with a feed-forward network. It uses residual connections and layer normalization to help with training stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadCausalAttention(config)\n",
        "        # TODO: Initialize layer normalization layers\n",
        "        # Hint: use nn.LayerNorm(config.d_model)\n",
        "        self.ln1 = ... # Your code here\n",
        "        self.ffn = FFN(config)\n",
        "        self.ln2 = ... # Your code here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement the forward pass with residual connections\n",
        "        # Remember the pattern: x = x + sublayer(layer_norm(x))\n",
        "        x = ... # Your code here  # First attention block with residual\n",
        "        x = ... # Your code here  # Second FFN block with residual\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = Config(d_model=256)\n",
        "ffn = FFN(config)\n",
        "decoder = DecoderBlock(config)\n",
        "\n",
        "# Test with random input\n",
        "x = torch.randn(2, 10, 256)  # (batch_size, sequence_length, d_model)\n",
        "output = decoder(x)\n",
        "assert output.shape == x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(config.d_model, 4*config.d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.l2 = nn.Linear(4*config.d_model, config.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.l2(x)\n",
        "        return x\n",
        "    \n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadCausalAttention(config)\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.ffn = FFN(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha(self.ln1(x))\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define the transformer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're now ready to put the components together into our final decoder module that can actually generate text! Your task to implement the missing pieces of the Decoder class. This is the top-level module that:\n",
        "\n",
        "* Embeds input tokens and adds positional information\n",
        "* Processes them through multiple transformer layers\n",
        "* Outputs predictions for the next token through the `forward(...)` function\n",
        "* Can generate new sequences autoregressively through the `generate(...)` function\n",
        "\n",
        "We have given extra hints for this module since it is a challenging exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Stack of decoder blocks\n",
        "        self.blocks = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layers)])\n",
        "\n",
        "        # TODO: Initialize components\n",
        "        # Final layer norm and projection to vocabulary\n",
        "        self.ln = ... # normalize to d_model dimension\n",
        "        self.lin = ... # project from d_model to vocab_size\n",
        "        \n",
        "        # Embeddings\n",
        "        self.wte = ... # Your code here, token embedding: vocab_size ‚Üí d_model\n",
        "        self.wpe = ... # Your code here, position embedding: ctx_len ‚Üí d_model\n",
        "        \n",
        "        # Loss function for training\n",
        "        self.L = nn.CrossEntropyLoss()\n",
        "        self.ctx_len = config.ctx_len\n",
        "\n",
        "        self.device = config.device # don't change this (for training model on right device)\n",
        "    \n",
        "    def forward(self, x, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tokens (B, T)\n",
        "            targets: Optional target tokens (B, T)\n",
        "        Returns:\n",
        "            logits: Predictions (B, T, vocab_size)\n",
        "            loss: Optional cross-entropy loss\n",
        "        \"\"\"\n",
        "        B, T = x.shape\n",
        "        \n",
        "        # TODO Step 1: Get embeddings\n",
        "        # Convert tokens to embeddings and add positional information\n",
        "        x_tok = self.wte(x)         # (B, T, d_model)\n",
        "        x_pos = self.wpe(torch.arange(T, device=self.device))        # (B, T, d_model)\n",
        "        x = ... # Your code here        # Add the embeddings together\n",
        "        \n",
        "        # TODO Step 2: Process through transformer\n",
        "        x = self.blocks(x)          # Apply transformer blocks\n",
        "        x = ... # Your code here        # Apply final layer norm\n",
        "        logits = ... # Your code here   # Project to vocabulary size\n",
        "        \n",
        "        # TODO Step 3: Compute loss if targets are provided\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape logits and targets for loss computation\n",
        "            B, T, V = logits.shape\n",
        "            logits = logits.view(B*T, V)    # Combine batch and time dimensions\n",
        "            targets = targets.view(B*T)      # Flatten targets\n",
        "            loss = ... # Your code here          # Compute cross entropy loss\n",
        "        \n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_len=256):\n",
        "        \"\"\"\n",
        "        Generate new tokens given initial sequence idx.\n",
        "        \"\"\"\n",
        "        # TODO: Implement generation loop\n",
        "        for _ in range(max_len):\n",
        "            # Step 1: Take the last ctx_len tokens\n",
        "            idx_window = ... # Your code here\n",
        "            \n",
        "            # Step 2: Get model predictions\n",
        "            logits, _ = self(idx_window)     # (B, T, V)\n",
        "            logits = logits[:, -1, :]        # Only take the last token's predictions\n",
        "            \n",
        "            # Step 3: Sample next token\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            \n",
        "            # Step 4: Append to sequence\n",
        "            idx = ... # Your code here\n",
        "        \n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 8])\n"
          ]
        }
      ],
      "source": [
        "config = Config(\n",
        "    vocab_size=100,\n",
        "    d_model=256,\n",
        "    ctx_len=64,\n",
        "    n_layers=4\n",
        ")\n",
        "decoder = Decoder(config)\n",
        "\n",
        "x = torch.randint(0, 100, (1, 10))\n",
        "logits, loss = decoder(x, x)\n",
        "\n",
        "out = decoder.generate(torch.tensor([[1, 2, 3]]), max_len=5)\n",
        "print(out.shape)  # Should be (1, 8) - original 3 tokens + 5 new ones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln = nn.LayerNorm(config.d_model)\n",
        "        self.lin = nn.Linear(config.d_model, config.vocab_size)\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.wpe = nn.Embedding(config.ctx_len, config.d_model)\n",
        "        self.L = nn.CrossEntropyLoss()\n",
        "        self.ctx_len = config.ctx_len\n",
        "        self.device = config.device\n",
        "    \n",
        "    def forward(self, x, targets=None):\n",
        "        B, T = x.shape\n",
        "        x_tok = self.wte(x)\n",
        "        x_pos = self.wpe(torch.arange(T, device=self.device))\n",
        "        x = x_tok + x_pos # (B, T, C)\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln(x)\n",
        "        logits = self.lin(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # compute xentropy loss, targets are (B, T)\n",
        "            B, T, V = logits.shape\n",
        "            targets = targets.view(B*T)\n",
        "            logits = logits.view(B*T, V)\n",
        "            loss = self.L(logits, targets)\n",
        "        \n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_len=256):\n",
        "        for _ in range(max_len):\n",
        "            idx_window = idx[:, -self.ctx_len:]\n",
        "            logits, _ = self(idx_window) #(B, T, V)\n",
        "            logits = logits[:,-1,:]\n",
        "            prob = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(prob, num_samples=1) # greedy sample\n",
        "            idx = torch.cat((idx, next_token), dim=1)\n",
        "        \n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total model parameters: 38988669\n",
            "step 0: train loss 6.9192, val loss 4.8996\n",
            "\to ÂÆâ K„Éá ·æ∂ oƒ∞Kp√ª «çÿ±ÿ°üáøÁï•·ºÄ√ëÈÄ£ŒØ‹º≈ûœÑ‚åø·ä≠êÄè≈Ω‚â§s–†—Ö s‚Üì√ñ ÿ≥r‡§øÿ© ‡¥∞ éoAI·∫ß ·ºΩ√¶—Öÿ°s·ÉîÊ•ö UÀπ‡¥≥–í   I‚àßƒÇÌåêüá∫√≠ƒ± N·ê±√ø(»ò„ÉÄ‚í∂ÂÆ∂  ”ôn +e¬∑‚ÜìŒ¶…í „ÉÉ √∑Œìo\n",
            "========================================\n",
            "step 200: train loss 2.5755, val loss 2.5723\n",
            "\t memedu ived tos bs anssthrishe apllg hiadely rlmardetethedenthtrngomosecortyelitinarolownge iedughe\n",
            "========================================\n",
            "step 400: train loss 2.5160, val loss 2.5505\n",
            "\t‚Äì blotier Rodean the smarandonelahid ock m, st m Jods, at asesoofrocanived, h ibedthas o m Ald 57851\n",
            "========================================\n",
            "step 600: train loss 2.4448, val loss 2.4834\n",
            "\trrac and sstuctor, intat Indsif for phot of thireroiggats outio\n",
            "U√Ö Id Derdengxpptic merceliatere) g \n",
            "========================================\n",
            "step 800: train loss 2.1160, val loss 2.1341\n",
            "\te‚Äâ), wich herered. The candes of Rusin Op, qual (1994) he my ray entriand. In danuanal casilalian, D\n",
            "========================================\n",
            "step 1000: train loss 1.7823, val loss 1.8958\n",
            "\tel comot invention enturoly puble conventer of bude in to the avery of buase. It long, chopening tra\n",
            "========================================\n",
            "step 1200: train loss 1.5664, val loss 1.7161\n",
            "\tnaFeas in Chio's Engles in War, Dayl Festle Singt Austria, II Busu diashoowled from Pligorts in the \n",
            "========================================\n",
            "step 1400: train loss 1.4967, val loss 1.5850\n",
            "\tIIII) Ex√©e+1\" thought to model the Shoutmon Dam (105) is the son without disarbordia, Dutch Champiod\n",
            "========================================\n",
            "step 1600: train loss 1.4182, val loss 1.5074\n",
            "\t nable (1; 15).\n",
            "  ; its nav√≠ARPRSW (1‚Äì15t)\n",
            " Villarian Laterarian Rigy\n",
            " Immajovan  íT. (168‚Äì1422, from\n",
            "========================================\n",
            "step 1800: train loss 1.3523, val loss 1.4730\n",
            "\t French emper of Islam and Julic species an anti-how super trade spoken and being by species to brea\n",
            "========================================\n",
            "step 2000: train loss 1.3601, val loss 1.4291\n",
            "\t hentiel is quadjald, when their gas.\n",
            "Were made stoppes a cellusion of a single bannasoverer and los\n",
            "========================================\n",
            "step 2200: train loss 1.2762, val loss 1.4090\n",
            "\t; Turban era, appear with 19 regulatives indicates that heliographical indicates in relationship suc\n",
            "========================================\n",
            "step 2400: train loss 1.2537, val loss 1.3849\n",
            "\t2:\n",
            "\n",
            "In a curb commer\n",
            "A groups in published prevalenting\n",
            "\n",
            " A regon\n",
            "\n",
            "Moist crowded moist regarding abb\n",
            "========================================\n",
            "step 2600: train loss 1.2424, val loss 1.3646\n",
            "\t a fearm. Saint also other were plans usoffers with the command firth, function to size K√∂nril Korer\n",
            "========================================\n",
            "step 2800: train loss 1.2433, val loss 1.3520\n",
            "\to had from Roman amuelo to munipate wtherefore and the balloogeneming system of Jakosos. This amused\n",
            "========================================\n",
            "step 3000: train loss 1.2646, val loss 1.3440\n",
            "\t undergo (No.m. 22), Phebeb Sandom ($150‚Äì731), Alabama's actualler, Milota (nochia), Milore, 319 (Ki\n",
            "========================================\n",
            "step 3200: train loss 1.1963, val loss 1.3243\n",
            "\t the den has sage for this rathean the church of the tangle, had been accessed abandoned from 1944 t\n",
            "========================================\n",
            "step 3400: train loss 1.2313, val loss 1.3210\n",
            "\t 10 haeme increase direct around Howler promites the angle is unmarred when the tow; rhotes of the a\n",
            "========================================\n",
            "step 3600: train loss 1.1907, val loss 1.3078\n",
            "\t tearned often summer and behaved decay meaning. There the notes are often known, 45% of the Revolut\n",
            "========================================\n",
            "step 3800: train loss 1.1462, val loss 1.2918\n",
            "\tpradinglands were a match of Frederick and a prime newer and independent constituents in Frederick c\n",
            "========================================\n",
            "step 4000: train loss 1.1829, val loss 1.2872\n",
            "\touncies Carban, that Hitchcock proven the first major fish onces to streal blue reacts bituming. Fis\n",
            "========================================\n",
            "step 4200: train loss 1.1896, val loss 1.2816\n",
            "\t\". The relationships were characteristic important in a subsidual proton that recomplex considered a\n",
            "========================================\n",
            "step 4400: train loss 1.1676, val loss 1.2757\n",
            "\tbity included that the sperry disciples of the subject almost well lay. Almost indiduces that arm ca\n",
            "========================================\n",
            "step 4600: train loss 1.0889, val loss 1.2732\n",
            "\tt F. Calmunist Betty, Antarctic tracks chose the man zogoginations around the state, which becomes n\n",
            "========================================\n",
            "step 4800: train loss 1.1153, val loss 1.2660\n",
            "\t Mes Eosible before, titled anarchist mechanical ideas, the poetic anarchist socialists cannot cause\n",
            "========================================\n",
            "\n",
            "Final generated text:\n",
            "\ted writing the challengest group priors taked in Spain, Œ¥„Çâ–ΩŒºÿß, –¥- which Ali will tok their prior sources.\"\n",
            "\n",
            "Head\n",
            "\"System of them everything\"\n",
            " A study of pointing\n",
            "\n",
            "The existence of point\n",
            "\n",
            "References of the palar moments of the history and contemporary cult (e.g., sandson). Thus the pairs which emission is also used to pave a System vehicle. The clerv. Glassion at the time, which are still to character them.\n",
            " \n",
            " \n",
            "The other lands sponsing the thirds condeida name is to a thrive, which is too used th\n"
          ]
        }
      ],
      "source": [
        "config = Config(d_model=512, n_heads=8, ctx_len=512, batch_size = 32, n_layers = 12, device='cuda:0')\n",
        "config.set_vocab_size(vocab_size=len(chars))\n",
        "model = Decoder(config).to(config.device)\n",
        "\n",
        "# print the size of the model\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total model parameters: {n_params}\")\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 3e-4\n",
        "max_iters = 5000\n",
        "eval_interval = 200  # How often to evaluate\n",
        "eval_iters = 100     # How many batches to use for evaluation\n",
        "\n",
        "# Adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    xb, yb = get_batch('train', config.ctx_len, config.batch_size, config.device)\n",
        "    \n",
        "    # forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "    \n",
        "    # backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # evaluate on val data at specified intervals\n",
        "    if iter % eval_interval == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_losses = []\n",
        "            for _ in range(eval_iters):\n",
        "                xb, yb = get_batch('val', config.ctx_len, config.batch_size, config.device)\n",
        "                _, val_loss = model(xb, yb)\n",
        "                val_losses.append(val_loss.item())\n",
        "            val_loss = sum(val_losses) / len(val_losses)\n",
        "            \n",
        "            print(f\"step {iter}: train loss {loss.item():.4f}, val loss {val_loss:.4f}\")\n",
        "            \n",
        "            # generate some text to see how we're doing\n",
        "            context = torch.zeros((1, 1), dtype=torch.long, device=config.device)\n",
        "            print(decode(model.generate(context, max_len=100)[0].tolist(), int_to_str))\n",
        "            print('='*40)\n",
        "        model.train()\n",
        "\n",
        "# Final generation\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=config.device)\n",
        "print(\"\\nFinal generated text:\")\n",
        "print(decode(model.generate(context, max_len=500)[0].tolist(), int_to_str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Roman Empire lasted from Anglican and made conditions the institutions sent in five April 1930 plants in the B√ºe do, Russian palanto. These conditions are often and often controlled agryment.\n",
            "\n",
            "Etymology\n",
            "\n",
            "Draduking \n",
            " Post-second claims and late ethical routine, some below is attrributed, indicated later neares 50% in the late 300s. The Prime Minister of Anglicanism that is allegated with new each latest climbers. The claim inscrients of equal to 100% with Nobel sign for 1.6% in 2008.\n",
            "\n",
            "Post-wars, post-relic intellectual resins in ne\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "context = torch.tensor([encode(\"The Roman Empire lasted from\", str_to_int)], dtype=torch.long, device=config.device)\n",
        "print(decode(model.generate(context, max_len=512)[0].tolist(), int_to_str))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
