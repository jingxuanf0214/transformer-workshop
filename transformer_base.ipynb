{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fcce1c8",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/KempnerInstitute/transformer-workshop/blob/main/transformer_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJjyDCYiycRG"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Yy3ncD8pycRH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbDLagMhycRH"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rgppY_zycRH"
   },
   "source": [
    "## Exercise: Implementing Character-based tokenization\n",
    "\n",
    "1. Get a sorted list of every unique character in your training data.\n",
    "2. Create a dictionary that converts tokens to IDs (str_to_int) and one that converts IDs to tokens (int_to_str)\n",
    "3. Implement functions encode and decode.\n",
    "Encode should take in a string and output list of token IDs.\n",
    "Decode should take in a list of token IDs and output a string.\n",
    "4. Test encoding and then decoding ‚ÄúMy dog Leo is extremely cute.‚Äù Do you recover the correct string?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JnmqJ-922AD_"
   },
   "outputs": [],
   "source": [
    "# Load in all training data\n",
    "with open('tiny_wikipedia.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pl73KMyZycRH",
    "outputId": "b7d9d16c-a7f6-4573-ceb6-1aec4a09cec0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1) Get a sorted list of all unique characters that occur in this text\n",
    "# Hint: set is useful for getting unique elements in a sequence\n",
    "... # your code here\n",
    "\n",
    "# Step 2) Create the dictionaries str_to_int and int_to_str\n",
    "... # your code here\n",
    "\n",
    "# Step 3) Define encode and decode functions\n",
    "# def encode(...):\n",
    "#     ...\n",
    "\n",
    "# def decode(...):\n",
    "#     ...\n",
    "\n",
    "# Testing your implementation \n",
    "input_text = \"My dog Leo is extremely cute.\"\n",
    "ids = encode(input_text, str_to_int)\n",
    "decoded_text = decode(ids, int_to_str)\n",
    "assert input_text == decoded_text, \"Decoded text does not match input\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint for step 1**\n",
    "\n",
    "Set is useful for getting unique elements in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pCQAN_QY3zPO"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# Step 1) Get a sorted list of all unique characters that occur in this text\n",
    "# Hint: set is useful for getting unique elements in a sequence\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# Step 2) Create the dictionaries str_to_int and int_to_str\n",
    "str_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_str = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Step 3) Define encode and decode functions\n",
    "def encode(text, str_to_int):\n",
    "    ids = [str_to_int[c] for c in text]\n",
    "    return ids\n",
    "\n",
    "def decode(ids, int_to_str):\n",
    "    text_list = [int_to_str[id] for id in ids]\n",
    "    return ''.join(text_list)\n",
    "\n",
    "# Testing your implementation \n",
    "input_text = \"My dog Leo is extremely cute.\"\n",
    "ids = encode(input_text, str_to_int)\n",
    "decoded_text = decode(ids, int_to_str)\n",
    "assert input_text == decoded_text, \"Decoded text does not match input\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUKsH64ZycRI"
   },
   "source": [
    "## Tokenize input data and create splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rVKYkgLuycRI"
   },
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text, str_to_int), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split, ctx_len, batch_size, device='cpu'):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - ctx_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+ctx_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+ctx_len+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our transformer parameters with a config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 256 # the model/hidden/embedding dim\n",
    "    n_heads: int = 4 # number of attention heads (width)\n",
    "    ctx_len: int = 64 # context length\n",
    "    batch_size: int = 8 # batch size\n",
    "    n_layers: int = 12 # number of layers (depth)\n",
    "    vocab_size: int = -1 # vocab size, to be determined once we have created a tokenizer\n",
    "    device: str = 'cpu'\n",
    "\n",
    "    def set_vocab_size(self, vocab_size):\n",
    "        self.vocab_size = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config.set_vocab_size(vocab_size=len(chars)) # set our vocabular size (equal to the number of chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Implement token embeddings\n",
    "\n",
    "We want to implement a class that will take in a batch of token IDs (batch size by context length) and output the token embeddings (batch size by context length by embedding dimensions). Find the `nn.Embedding` docs [here](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: fill out nn.Embedding arguments for token embedding\n",
    "        self.wte = nn.Embedding(...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # TODO: Get forward pass of token embedding layer\n",
    "        x_tok = ...\n",
    "\n",
    "        return x_tok\n",
    "    \n",
    "\n",
    "# Testing your implementation\n",
    "xb, yb = get_batch('train', config.ctx_len, config.batch_size, config.device)\n",
    "\n",
    "token_embedding = TokenEmbeddingLayer(config)\n",
    "x_tok = token_embedding(xb)\n",
    "\n",
    "assert x_tok.shape == (config.batch_size, config.ctx_len, config.d_model), \"Embedding dimensions are incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "class TokenEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.wte = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        \n",
    "        x_tok = self.wte(x)\n",
    "\n",
    "        return x_tok\n",
    "    \n",
    "\n",
    "# Testing your implementation\n",
    "xb, yb = get_batch('train', config.ctx_len, config.batch_size, config.device)\n",
    "\n",
    "token_embedding = TokenEmbeddingLayer(config)\n",
    "x_tok = token_embedding(xb)\n",
    "\n",
    "assert x_tok.shape == (config.batch_size, config.ctx_len, config.d_model), \"Embedding dimensions are incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Implement full embedding layer\n",
    "\n",
    "Now we'll write the full embedding layer including both token and position embeddings. You can use your solutions from the previous part for the token embeddings. How can you implement the position embeddings? This is a little tricky so feel free to click on the hints below the exercise for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = config.device\n",
    "\n",
    "        # TODO: fill out nn.Embedding arguments for token embedding (same as before)\n",
    "        self.wte = nn.Embedding(...)\n",
    "\n",
    "        # TODO: implement position embedding\n",
    "        self.wpe = ...\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # TODO: Get forward pass of token and position embedding \n",
    "        x_tok = ...\n",
    "        x_pos = ...\n",
    "        x_embeddings = x_tok + x_pos \n",
    "\n",
    "        return x_embeddings\n",
    "    \n",
    "\n",
    "# Testing your implementation\n",
    "xb, yb = get_batch('train', config.ctx_len, config.batch_size, config.device)\n",
    "\n",
    "embedding = EmbeddingLayer(config)\n",
    "x_embedding = embedding(xb)\n",
    "\n",
    "assert x_embedding.shape == (config.batch_size, config.ctx_len, config.d_model), \"Embedding dimensions are incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint #1**\n",
    "\n",
    "For position embeddings, you can also use nn.Encoding. Instead of the first dimension being equal to vocab size, it should be equal to the context length (so you learn an embedding for each position in a sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint #2**\n",
    "\n",
    "The output of the token embeddings forward pass is batch size x context length x model dimension.\n",
    "\n",
    "For the forward pass of the position embeddings, you only need to create a matrix that is context length by model dimension as nothing depends on the actual data in each batch. Broadcasting will ensure you can still add this matrix to the token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.wte = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.wpe = nn.Embedding(config.ctx_len, config.d_model)\n",
    "        self.device = config.device\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        \n",
    "        x_tok = self.wte(x)\n",
    "        # print(x_tok.shape) uncomment this if you want to see the shape of above tensor\n",
    "        x_pos = self.wpe(torch.arange(T, device=self.device))\n",
    "        # print(x_pos.shape)\n",
    "        x_embeddings = x_tok + x_pos \n",
    "\n",
    "        return x_embeddings\n",
    "    \n",
    "\n",
    "# Testing your implementation\n",
    "xb, yb = get_batch('train', config.ctx_len, config.batch_size, config.device)\n",
    "\n",
    "embedding = EmbeddingLayer(config)\n",
    "x_embedding = embedding(xb)\n",
    "\n",
    "assert x_embedding.shape == (config.batch_size, config.ctx_len, config.d_model), \"Embedding dimensions are incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbNpCKoSycRI"
   },
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Implementing single headed causal self attention\n",
    "\n",
    "Self-attention is a core mechanism in transformers that allows each position in a sequence to attend to all previous positions. The \"causal\" part ensures each position can only attend to past positions - this is crucial for language modeling.\n",
    "\n",
    "The task below is to fill out the `SingleHeadCausalAttention` module.  The `__init__` method should define the key, query, and value projections.  Note that the causal mask has been already defined for you (it is a lower triangular matrix whose entries are 1's.  You can refer to it by calling `self.cmask`.)\n",
    "\n",
    "The `forward(self, x)` function that will take in an input `x` that is `(B, T, C)`-dimensional corresponding to batch size, sequence length, and hidden dimension and then output the result after applying the attention formula.  To do this,\n",
    "\n",
    "1. Create the K, Q, V matrices that are the resultant matrices after applying the `self.key`, `self.query`, and `self.values` projections.\n",
    "2. Compute and return attention using the formula:\n",
    "\n",
    "$$\\textrm{attention}(K, V, Q) = \\textrm{softmax}\\left( c \\odot \\frac{Q K^\\top}{\\sqrt{d_k}} \\right) V $$\n",
    "\n",
    "where $c \\odot \\dots$ denotes the application of the causal mask.  You can use `torch.masked_fill(...)` here to apply the mask.  It takes as input three arguments: the input matrix you want to mask, where you want to mask it (a boolean condition), and the value you want to mask with.  To figure out what value you want to mask with, it may be helpful to recall the softmax formula; the $i$-th component of a vector $u$ after a softmax is: $$ \\textrm{softmax}(x)_i =  \\frac{e^{x_i}}{\\sum_j e^{x_j}}.$$\n",
    "\n",
    "Hints:\n",
    "1. Keep track of the matrix dimensions after each step!\n",
    "2. Note that you can transpose a matrix in Pytorch by calling `A.transpose(dim_1, dim_2)` where `dim_1`, `dim_2` refer to the dimensions you want to transpose.\n",
    "3. You may use Pytorch's built-in softmax function `F.softmax(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadCausalAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calculate the dimension for each attention head\n",
    "        self.head_dim = config.d_model // config.n_heads\n",
    "        \n",
    "        # TODO: Initialize the Key, Query, and Value projections\n",
    "        # Each should be a linear layer that projects from d_model to head_dim\n",
    "        # Hint: Use nn.Linear(..., bias=False) as is standard in attention\n",
    "        self.key = ... # Your code here\n",
    "        self.query = ... # Your code here\n",
    "        self.values = ... # Your code here\n",
    "        \n",
    "        # Create causal mask (lower triangular matrix), you an refer to it by `self.cmask`\n",
    "        self.register_buffer(\"cmask\", torch.tril(torch.ones([config.ctx_len, config.ctx_len])))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # TODO Step 1: Project input to get Key, Query, Value matrices\n",
    "        K = ... # Your code here\n",
    "        Q = ... # Your code here\n",
    "        V = ... # Your code here\n",
    "        \n",
    "        # TODO Step 2: Compute attention scores and apply mask\n",
    "        # Remember: \n",
    "        # - Scale by sqrt(head_dim)\n",
    "        # - Use the causal mask (self.cmask) to prevent attention to future tokens (you can use `torch.masked_fill(...)` here)\n",
    "        # - Apply softmax to get attention weights\n",
    "        # - Multiply with values\n",
    "        \n",
    "        ... # Your implementation here...\n",
    "        \n",
    "        return # Final output\n",
    "    \n",
    "\n",
    "# Test your implementation\n",
    "config = Config(d_model=256, n_heads=8, ctx_len=16)\n",
    "attention = SingleHeadCausalAttention(config)\n",
    "x = torch.randn(2, 10, 256)  # (batch_size, seq_len, d_model)\n",
    "output = attention(x)\n",
    "assert output.shape == (2, 10, 32)  # head_dim = 256/8 = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "class SingleHeadCausalAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_dim = config.d_model // config.n_heads\n",
    "        self.key = nn.Linear(config.d_model, self.head_dim, bias=False)\n",
    "        self.query = nn.Linear(config.d_model, self.head_dim, bias=False)\n",
    "        self.values = nn.Linear(config.d_model, self.head_dim, bias=False)\n",
    "\n",
    "        self.register_buffer(\"cmask\", torch.tril(torch.ones([config.ctx_len, config.ctx_len])))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        K = self.key(x) # (B, T, C) @ (_, C, H) -> (B, T, H)\n",
    "        Q = self.query(x)\n",
    "        V = self.values(x)\n",
    "\n",
    "        y = Q @ K.transpose(-2, -1) * self.head_dim**-0.5 # (B, T, H) @ (B, H, T) -> (B, T, T)\n",
    "        y = torch.masked_fill(y, self.cmask[:T, :T]==0, float('-inf'))\n",
    "        y = F.softmax(y, dim=-1) @ V\n",
    "        return y\n",
    "    \n",
    "\n",
    "# Test your implementation\n",
    "config = Config(d_model=256, n_heads=8, ctx_len=16)\n",
    "attention = SingleHeadCausalAttention(config)\n",
    "x = torch.randn(2, 10, 256)  # (batch_size, seq_len, d_model)\n",
    "output = attention(x)\n",
    "assert output.shape == (2, 10, 32)  # head_dim = 256/8 = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: implementing multi-head attention\n",
    "\n",
    "The task is to write the multi-headed self attention module.  You should not need to write more than a few lines of code here.\n",
    "\n",
    "1. Define `self.heads` as the list of attention heads that will act in parallel on the input.  You may use `nn.ModuleList(...)` to do this.\n",
    "2. Define `self.linear`, a linear projection.\n",
    "3. Define the forward function which will take in the input `x` (which is (B, T, C)-dimesional), pass it through each head, and concatenate the output.  To perform concatenation you can use `torch.cat(...)`.\n",
    "4. After going through the attention heads, the input should then go through the linear projection and then returned at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = ... # your code here, you can define the heads using `nn.ModuleList(...)`\n",
    "        self.linear = ... # your code here, with `d_model` in-features and `d_model` out-features\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: fill out the forward method for multi-head attention\n",
    "        # Remember:\n",
    "        # - pass input through all heads and concatenate the output (you can use `torch.cat(...)` here)\n",
    "        # - pass the result through the linear layer and return the output\n",
    "        \n",
    "        ... # your code here\n",
    "\n",
    "\n",
    "# Testing your implementation\n",
    "config = Config(d_model=256, n_heads=8, ctx_len=16)\n",
    "mha = MultiHeadCausalAttention(config)\n",
    "\n",
    "# Test with small batch\n",
    "x = torch.randn(2, 10, 256)  # (batch_size=2, seq_len=10, d_model=256)\n",
    "out = mha(x)\n",
    "assert out.shape == (2, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "class MultiHeadCausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SingleHeadCausalAttention(config) for _ in range(config.n_heads)])\n",
    "        self.linear = nn.Linear(config.d_model, config.d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        y = self.linear(y)\n",
    "        return y\n",
    "    \n",
    "\n",
    "# Testing your implementation\n",
    "config = Config(d_model=256, n_heads=8, ctx_len=16)\n",
    "mha = MultiHeadCausalAttention(config)\n",
    "\n",
    "# Test with small batch\n",
    "x = torch.randn(2, 10, 256)  # (batch_size=2, seq_len=10, d_model=256)\n",
    "out = mha(x)\n",
    "assert out.shape == (2, 10, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the feed-forward network (FFN) decoder block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: FFN\n",
    "\n",
    "The Feed-Forward Network (FFN) is a simple yet powerful component that applies two linear transformations with a ReLU activation in between. The first transformation expands the input dimension by a factor of 4, and the second transformation projects it back to the original dimension.  In this exercise, you will implement this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # TODO: Initialize two linear layers\n",
    "        # First layer should expand from d_model to 4*d_model\n",
    "        # Second layer should project back to d_model\n",
    "        # Hint: use nn.Linear(in_features, out_features)\n",
    "        self.l1 = ... # Your code here\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = ... # Your code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        # 1. Apply first linear layer\n",
    "        # 2. Apply ReLU activation\n",
    "        # 3. Apply second linear layer\n",
    "        x = ... # Your code here\n",
    "        x = ... # Your code here\n",
    "        x = ... # Your code here\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(config.d_model, 4*config.d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(4*config.d_model, config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Decoder Block\n",
    "\n",
    "The Decoder Block is a core component that combines self-attention with a feed-forward network. It uses residual connections and layer normalization to help with training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadCausalAttention(config)\n",
    "        # TODO: Initialize layer normalization layers\n",
    "        # Hint: use nn.LayerNorm(config.d_model)\n",
    "        self.ln1 = ... # Your code here\n",
    "        self.ffn = FFN(config)\n",
    "        self.ln2 = ... # Your code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass with residual connections\n",
    "        # Remember the pattern: x = x + sublayer(layer_norm(x))\n",
    "        x = ... # Your code here  # First attention block with residual\n",
    "        x = ... # Your code here  # Second FFN block with residual\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Testing your implementation\n",
    "config = Config(d_model=256)\n",
    "ffn = FFN(config)\n",
    "decoder = DecoderBlock(config)\n",
    "\n",
    "# Test with random input\n",
    "x = torch.randn(2, 10, 256)  # (batch_size, sequence_length, d_model)\n",
    "output = decoder(x)\n",
    "assert output.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadCausalAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.d_model)\n",
    "        self.ffn = FFN(config)\n",
    "        self.ln2 = nn.LayerNorm(config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Testing your implementation\n",
    "config = Config(d_model=256)\n",
    "ffn = FFN(config)\n",
    "decoder = DecoderBlock(config)\n",
    "\n",
    "# Test with random input\n",
    "x = torch.randn(2, 10, 256)  # (batch_size, sequence_length, d_model)\n",
    "output = decoder(x)\n",
    "assert output.shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the transformer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to put the components together into our final decoder module that can actually generate text! Your task to implement the missing pieces of the Decoder class. This is the top-level module that:\n",
    "\n",
    "* Embeds input tokens and adds positional information\n",
    "* Processes them through multiple transformer layers\n",
    "* Outputs predictions for the next token through the `forward(...)` function\n",
    "* Can generate new sequences autoregressively through the `generate(...)` function\n",
    "\n",
    "We have given extra hints for this module since it is a challenging exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Stack of decoder blocks\n",
    "        self.blocks = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layers)])\n",
    "\n",
    "        # TODO: Initialize components\n",
    "        # Final layer norm and projection to vocabulary\n",
    "        self.ln = ... # normalize to d_model dimension\n",
    "        self.lin = ... # project from d_model to vocab_size\n",
    "        \n",
    "        # Embeddings\n",
    "        self.emb = EmbeddingLayer(config)\n",
    "\n",
    "        # Loss function for training\n",
    "        self.L = nn.CrossEntropyLoss()\n",
    "        self.ctx_len = config.ctx_len\n",
    "\n",
    "        self.device = config.device # don't change this (for training model on right device)\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tokens (B, T)\n",
    "            targets: Optional target tokens (B, T)\n",
    "        Returns:\n",
    "            logits: Predictions (B, T, vocab_size)\n",
    "            loss: Optional cross-entropy loss\n",
    "        \"\"\"\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # Step 1: Get embeddings\n",
    "        x = self.emb(x) \n",
    "        \n",
    "        # TODO Step 2: Process through transformer\n",
    "        x = self.blocks(x)          # Apply transformer blocks\n",
    "        x = ... # Your code here        # Apply final layer norm\n",
    "        logits = ... # Your code here   # Project to vocabulary size\n",
    "        \n",
    "        # TODO Step 3: Compute loss if targets are provided\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshape logits and targets for loss computation\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)    # Combine batch and time dimensions\n",
    "            targets = targets.view(B*T)      # Flatten targets\n",
    "            loss = ... # Your code here          # Compute cross entropy loss\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_len=256):\n",
    "        \"\"\"\n",
    "        Generate new tokens given initial sequence idx.\n",
    "        \"\"\"\n",
    "        # TODO: Implement generation loop\n",
    "        for _ in range(max_len):\n",
    "            # Step 1: Take the last ctx_len tokens\n",
    "            idx_window = ... # Your code here\n",
    "            \n",
    "            # Step 2: Get model predictions\n",
    "            logits, _ = self(idx_window)     # (B, T, V)\n",
    "            logits = logits[:, -1, :]        # Only take the last token's predictions\n",
    "            \n",
    "            # Step 3: Sample next token\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Step 4: Append to sequence\n",
    "            idx = ... # Your code here\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "\n",
    "# Testing your implementation\n",
    "config = Config(\n",
    "    vocab_size=100,\n",
    "    d_model=256,\n",
    "    ctx_len=64,\n",
    "    n_layers=4\n",
    ")\n",
    "decoder = Decoder(config)\n",
    "\n",
    "x = torch.randint(0, 100, (1, 10))\n",
    "logits, loss = decoder(x, x)\n",
    "\n",
    "out = decoder.generate(torch.tensor([[1, 2, 3]]), max_len=5)\n",
    "print(out.shape)  # Should be (1, 8) - original 3 tokens + 5 new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layers)])\n",
    "        self.ln = nn.LayerNorm(config.d_model)\n",
    "        self.lin = nn.Linear(config.d_model, config.vocab_size)\n",
    "        self.emb = EmbeddingLayer(config)\n",
    "        self.L = nn.CrossEntropyLoss()\n",
    "        self.ctx_len = config.ctx_len\n",
    "        self.device = config.device\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "        x = self.emb(x)\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.lin(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # compute xentropy loss, targets are (B, T)\n",
    "            B, T, V = logits.shape\n",
    "            targets = targets.view(B*T)\n",
    "            logits = logits.view(B*T, V)\n",
    "            loss = self.L(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_len=256):\n",
    "        for _ in range(max_len):\n",
    "            idx_window = idx[:, -self.ctx_len:]\n",
    "            logits, _ = self(idx_window) #(B, T, V)\n",
    "            logits = logits[:,-1,:]\n",
    "            prob = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(prob, num_samples=1) # greedy sample\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "\n",
    "# Testing your implementation\n",
    "config = Config(\n",
    "    vocab_size=100,\n",
    "    d_model=256,\n",
    "    ctx_len=64,\n",
    "    n_layers=4\n",
    ")\n",
    "decoder = Decoder(config)\n",
    "\n",
    "x = torch.randint(0, 100, (1, 10))\n",
    "logits, loss = decoder(x, x)\n",
    "\n",
    "out = decoder.generate(torch.tensor([[1, 2, 3]]), max_len=5)\n",
    "print(out.shape)  # Should be (1, 8) - original 3 tokens + 5 new ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters: 86794109\n",
      "step 0: train loss 7.0461, val loss 4.8023\n",
      "\ti-÷∞aƒåŒóe m ÃΩ  »ô Ÿá¬´  e ‚Äê …ô‚çµaÀ∫  ‚Äâ ¬∞eh œéi a  »òR◊ë‚Üì«ª◊† üá¶ Œ± ·ΩÅ s  ≈õ ƒê·π≠ i ƒû „Éì  ·Éê  √¥ B t   ‚Ä¶a ‹ó     ·∂è 9·∏≤ „Åç·∫Æ‚ô¶ —â‡¶ï\n",
      "========================================\n",
      "step 200: train loss 2.5676, val loss 2.5627\n",
      "\t ‚Äì Do \" E3œÉ mybanged, Larimivernns fomopluss Vistid E)\n",
      " tencal wasof perl cuererernellk thees p ks. \n",
      "========================================\n",
      "step 400: train loss 2.5312, val loss 2.5347\n",
      "\t80demmect prengalologes ty coffrky, taved oniowhatenchon ariccepeithiong \"crs s on pumathemm cowke o\n",
      "========================================\n",
      "step 600: train loss 2.2975, val loss 2.3220\n",
      "\ten Fors hage.\n",
      "\n",
      "okhat \n",
      "\n",
      "10sBe wito dit am thent derupition Sey ated of the ALS Aris 21. M thilm efou:\n",
      "========================================\n",
      "step 800: train loss 1.8219, val loss 1.8791\n",
      "\tumely bused then SDire islanda in Ausconod MCory used more sastement used of the the Sciet, includin\n",
      "========================================\n",
      "step 1000: train loss 1.5887, val loss 1.6315\n",
      "\todulling Laigeboggh compared Japs, 1975, Japanese Murillen V. 1963. North Scott. Homparrader, Greall\n",
      "========================================\n",
      "step 1200: train loss 1.3900, val loss 1.4763\n",
      "\t sadon, ince he adopted, which semiss were describely that new, who away succested because a term No\n",
      "========================================\n",
      "step 1400: train loss 1.2799, val loss 1.4150\n",
      "\t advanced rocket book, freed level in nuning different impressed the project of security between agr\n",
      "========================================\n",
      "step 1600: train loss 1.2738, val loss 1.3706\n",
      "\t Body saw languages with more and foltenward;  1896 thought undercates through the relate age fifte \n",
      "========================================\n",
      "step 1800: train loss 1.2286, val loss 1.3458\n",
      "\tudal, Los√©a messenger Cosumbine, Vol. Thompso, Preso, Phig Addena, Maqazi Piny, Northdon's Rude\n",
      "The \n",
      "========================================\n",
      "step 2000: train loss 1.2084, val loss 1.3133\n",
      "\teun since 1954 tutorio.\n",
      "\n",
      "Usage of the first mathematical school\" occupy are an allore that means tha\n",
      "========================================\n",
      "step 2200: train loss 1.1877, val loss 1.2927\n",
      "\tBig of As of Cliffsors (can be wread in 1983) and Athens Vrd hinting soldiers. After the another, th\n",
      "========================================\n",
      "step 2400: train loss 1.1376, val loss 1.2831\n",
      "\t, Chicagores\n",
      " Vihibi, \n",
      " Achilles of Allia by Jamans\n",
      "\n",
      "Flowing on the first Allianos\n",
      " \"Independent int\n",
      "========================================\n",
      "step 2600: train loss 1.1043, val loss 1.2681\n",
      "\t\n",
      " It for Catalynok de Golden Oscars, who wordled to receive latter to drop couvering home, by use in\n",
      "========================================\n",
      "step 2800: train loss 1.1158, val loss 1.2564\n",
      "\t Sam, winns, and Samatie, people massacre modes, and holding most of 2023‚Äì1974; the common telephone\n",
      "========================================\n",
      "step 3000: train loss 1.0848, val loss 1.2543\n",
      "\t500¬†km/w-12¬†‚Äì12¬†¬∞–ø¬†W‰∏â: Coe:¬†HB¬†‚Äì CCID 139¬†‚Äì132 TCIO:12]¬†‚Äì CB, CIDID 141 Sea suffragatus L–æC (ML)1 pt\n",
      "========================================\n",
      "step 3200: train loss 1.0405, val loss 1.2465\n",
      "\t-.. In 1855 and the more priced upon the traditional remains for the District, after designed its AR\n",
      "========================================\n",
      "step 3400: train loss 1.0550, val loss 1.2473\n",
      "\t.Œë. The range construed his policy fires and suffered an ending international unit affining the Cold\n",
      "========================================\n",
      "step 3600: train loss 0.9843, val loss 1.2422\n",
      "\told Frie in 1983, Mmin Vincent Press, Brazil Lord Unsult (1984), a man in several collections of Net\n",
      "========================================\n",
      "step 3800: train loss 1.0217, val loss 1.2360\n",
      "\tth.D.Sligalb Aldock \n",
      "In a more radical and quarter section via Osidoro, Eplas Pfake departs an abbre\n",
      "========================================\n",
      "step 4000: train loss 1.0191, val loss 1.2345\n",
      "\t   ‚Äì Owen Young Winkamer (IST): Polish, Junky 1999 ‚Äì to issue a prophet, killing the autonomy left a\n",
      "========================================\n",
      "step 4200: train loss 0.9847, val loss 1.2354\n",
      "\t female enemies are found to be empirically meaning \n",
      " Spielboat, certain appets certain part-museums\n",
      "========================================\n",
      "step 4400: train loss 0.9486, val loss 1.2459\n",
      "\t facilities overpointed her G. O. Dee (ABBA)\", which are Interior marine to Asia Senate, Malaysia, S\n",
      "========================================\n",
      "step 4600: train loss 0.9281, val loss 1.2480\n",
      "\t6.5Ld alkyl group and writing 280 subtle web, due to their heads, impression and ten time. In 1944, \n",
      "========================================\n",
      "step 4800: train loss 0.9259, val loss 1.2596\n",
      "\t Notorious Friends' White Weekly Persons? ranked 13th individuals without architectural field as a t\n",
      "========================================\n",
      "step 5000: train loss 0.8942, val loss 1.2628\n",
      "\t2003, ABBA's light up may not be, but is now only generally. The record was established by many othe\n",
      "========================================\n",
      "step 5200: train loss 0.9160, val loss 1.2695\n",
      "\t œÉ \\ professional inverses in D. The Engine and Women in D. Muels has been available that the new st\n",
      "========================================\n",
      "step 5400: train loss 0.8533, val loss 1.2807\n",
      "\tLwriterary work Racing published about fabrice using the traditions for being termed sixteen (132‚Äì10\n",
      "========================================\n",
      "step 5600: train loss 0.8656, val loss 1.2825\n",
      "\t+ and the size of moistly differed variations of this is pointed out.  It is well about becoming in \n",
      "========================================\n",
      "step 5800: train loss 0.8664, val loss 1.3100\n",
      "\tŒ¥Œªje–ö ·ºå √°Œ∑œÇ·∫≤êåÄ‡¥á√≥ra·µÑ ‚áå—Ö–∏œÑŒπŒæŒøœÉŒØ ·øÉ–µŒî–î–µ–∑ œÑ–∞/ estimate of work (not to be met internal and external creoli\n",
      "========================================\n",
      "\n",
      "Final generated text:\n",
      "\t, huck the victory the kinship of Perso, is covered by hunckeping Gershart Horses who spoke with hugs and over the copic. Baku also is a character of flectuations, who sailed to restore the top roots of the Baku‚ÄìTbite in 1966.\n",
      "\n",
      "Early Temple some marriage was fully established, including a lime locality either the support of hunting or one. Not admitted that God's beliefs and the royar were not possible to transport of these fabulae. We f·ªÖlow 'astronaut astronaut '\" could not be competed. Temples\n"
     ]
    }
   ],
   "source": [
    "config = Config(d_model=768, n_heads=12, ctx_len=512, batch_size = 64, n_layers = 12, device='cuda:0')\n",
    "config.set_vocab_size(vocab_size=len(chars))\n",
    "model = Decoder(config).to(config.device)\n",
    "\n",
    "# print the size of the model\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total model parameters: {n_params}\")\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 3e-4\n",
    "max_iters = 6000\n",
    "eval_interval = 200  # How often to evaluate\n",
    "eval_iters = 100     # How many batches to use for evaluation\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    xb, yb = get_batch('train', config.ctx_len, config.batch_size, config.device)\n",
    "    \n",
    "    # forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    # backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # evaluate on val data at specified intervals\n",
    "    if iter % eval_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for _ in range(eval_iters):\n",
    "                xb, yb = get_batch('val', config.ctx_len, config.batch_size, config.device)\n",
    "                _, val_loss = model(xb, yb)\n",
    "                val_losses.append(val_loss.item())\n",
    "            val_loss = sum(val_losses) / len(val_losses)\n",
    "            \n",
    "            print(f\"step {iter}: train loss {loss.item():.4f}, val loss {val_loss:.4f}\")\n",
    "            \n",
    "            # generate some text to see how we're doing\n",
    "            context = torch.zeros((1, 1), dtype=torch.long, device=config.device)\n",
    "            print(decode(model.generate(context, max_len=100)[0].tolist(), int_to_str))\n",
    "            print('='*40)\n",
    "        model.train()\n",
    "\n",
    "# Final generation\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=config.device)\n",
    "print(\"\\nFinal generated text:\")\n",
    "print(decode(model.generate(context, max_len=500)[0].tolist(), int_to_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our model\n",
    "import os\n",
    "if not os.path.exists('model.pth'):\n",
    "   torch.save(model.state_dict(), 'model.pth')\n",
    "else:\n",
    "   print('Model file (model.pth) already exists!  Saving under a different name model_other.pth.')\n",
    "   torch.save(model.state_dict(), 'model_other.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Roman Empire lasted from The Objectivist states international use of diplomatic animals, Turkish early Discovery earned him a switch about the creation of the Thirteenth Century. Theologians caught little prisms, long have become one of the diseases occupying humane either being curification or humans. Turkish systems can act as animated subjective tensor, symbols and elastic in comparison to computers in previous biographies and neighboring skin.\n",
      "\n",
      "Bibliography\n",
      "\n",
      "\"The birth of animals, walkes and the choice of animals, right, arche\n"
     ]
    }
   ],
   "source": [
    "# to load a saved model, uncomment the below code\n",
    "# model = Decoder(config)\n",
    "# model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "config.device = 'cuda:0'  # change this to 'cpu' if you don't have access to a GPU\n",
    "model.eval()\n",
    "context = torch.tensor([encode(\"The Roman Empire lasted from\", str_to_int)], dtype=torch.long, device=config.device)\n",
    "print(decode(model.generate(context, max_len=512)[0].tolist(), int_to_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer models are described by shear when the quality of the two mass-powered Reflective Reflectivity Metal sided for this reason surrounding the measures because of their health beyond mass currents.\n",
      "\n",
      "The formulas of the interaction of the number of variations require in cilia, with each of surviving sodium in external water. The silver representation with reaction and basal organization. This circulation is either related to the Armenian genocide, with the secondary equivalent to title or the costs of two percent in the origins of Cal\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([encode(\"Transformer models are described by\", str_to_int)], dtype=torch.long, device=config.device)\n",
    "print(decode(model.generate(context, max_len=512)[0].tolist(), int_to_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
